# Enhanced Flow Matching for Super Mario Bros with Action Conditioning
# ä½¿ç”¨data_marioæ•°æ®è®­ç»ƒï¼Œæ”¯æŒåŠ¨ä½œæ¡ä»¶ç”Ÿæˆå’Œåºåˆ—ç”Ÿæˆ

import math
import os
import time
import re
import struct
from dataclasses import dataclass
from typing import List, Tuple, Optional

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import torchvision
from torchvision import transforms
from torchvision.utils import save_image

import matplotlib.pyplot as plt
import numpy as np
from PIL import Image
import imageio

# -----------------------------
# Config
# -----------------------------
@dataclass
class Config:
    image_size: int = 256      # 256x240 -> 256x256
    in_ch: int = 3             # RGB
    base_ch: int = 64          # å‡å°‘åŸºç¡€é€šé“æ•°ä»¥é€‚åº”GPUå†…å­˜
    num_actions: int = 256     # 8ä½åŠ¨ä½œç¼–ç  (0-255)
    
    batch_size: int = 4        # å‡å°‘æ‰¹æ¬¡å¤§å°ä»¥é€‚åº”GPUå†…å­˜
    epochs: int = 20
    lr: float = 1e-4           # é™ä½å­¦ä¹ ç‡
    num_samples: int = 4
    
    grad_clip: float = 1.0
    ode_steps: int = 100
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    
    # GPUä¼˜åŒ–å‚æ•°
    mixed_precision: bool = True  # æ··åˆç²¾åº¦è®­ç»ƒ
    num_workers: int = 8          # æ•°æ®åŠ è½½å·¥ä½œè¿›ç¨‹æ•°
    pin_memory: bool = True       # å†…å­˜å›ºå®š
    out_dir: str = "./output/enhanced_mario"
    data_path: str = "./data_mario"
    
    # åºåˆ—ç”Ÿæˆå‚æ•°
    sequence_length: int = 10  # ç”Ÿæˆåºåˆ—é•¿åº¦
    context_length: int = 5    # ä¸Šä¸‹æ–‡é•¿åº¦

cfg = Config()
os.makedirs(cfg.out_dir, exist_ok=True)

# -----------------------------
# Custom Dataset for Mario Data
# -----------------------------
class MarioDataset(Dataset):
    def __init__(self, data_path: str, image_size: int = 256):
        self.data_path = data_path
        self.image_size = image_size
        self.image_files = []
        self.actions = []
        
        # åŠ è½½æ‰€æœ‰PNGæ–‡ä»¶
        self._load_data()
        
        # æ•°æ®å˜æ¢
        self.transform = transforms.Compose([
            transforms.Resize((image_size, image_size)),
            transforms.ToTensor(),
            transforms.Normalize(0.5, 0.5),  # [-1, 1]
        ])
    
    def _load_data(self):
        """åŠ è½½æ‰€æœ‰PNGæ–‡ä»¶å’Œå¯¹åº”çš„åŠ¨ä½œ"""
        print(f"ğŸ” æ­£åœ¨æ‰«ææ•°æ®è·¯å¾„: {self.data_path}")
        
        if not os.path.exists(self.data_path):
            print(f"âŒ æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {self.data_path}")
            return
        
        total_files = 0
        valid_files = 0
        
        for root, dirs, files in os.walk(self.data_path):
            print(f"ğŸ“ æ‰«æç›®å½•: {root}")
            print(f"   æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶")
            
            for file in files:
                total_files += 1
                if file.endswith('.png'):
                    file_path = os.path.join(root, file)
                    action = self._extract_action_from_filename(file)
                    if action is not None:
                        self.image_files.append(file_path)
                        self.actions.append(action)
                        valid_files += 1
                    else:
                        print(f"âš ï¸  æ— æ³•æå–åŠ¨ä½œ: {file}")
        
        print(f"ğŸ“Š æ‰«æç»“æœ:")
        print(f"   æ€»æ–‡ä»¶æ•°: {total_files}")
        print(f"   æœ‰æ•ˆå›¾åƒ: {valid_files}")
        print(f"âœ… åŠ è½½äº† {len(self.image_files)} å¼ å›¾åƒ")
        
        if len(self.actions) > 0:
            print(f"   åŠ¨ä½œåˆ†å¸ƒ: {np.bincount(self.actions, minlength=256)[:20]}...")  # æ˜¾ç¤ºå‰20ä¸ªåŠ¨ä½œçš„åˆ†å¸ƒ
        else:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„å›¾åƒæ–‡ä»¶")
    
    def _extract_action_from_filename(self, filename: str) -> Optional[int]:
        """ä»æ–‡ä»¶åæå–åŠ¨ä½œç¼–ç """
        # æ–‡ä»¶åæ ¼å¼: Rafael_dp2a9j4i_e6_1-1_f1000_a20_2019-04-13_20-13-16.win.png
        pattern = r'_a(\d+)_'
        match = re.search(pattern, filename)
        if match:
            return int(match.group(1))
        return None
    
    def __len__(self):
        return len(self.image_files)
    
    def __getitem__(self, idx):
        # åŠ è½½å›¾åƒ
        image_path = self.image_files[idx]
        image = Image.open(image_path).convert('RGB')
        image = self.transform(image)
        
        # è·å–åŠ¨ä½œ
        action = self.actions[idx]
        
        return image, action

# -----------------------------
# Enhanced Time Embedding
# -----------------------------
def sinusoidal_time_embedding(t, dim):
    """æ”¹è¿›çš„æ—¶é—´åµŒå…¥"""
    device = t.device
    half = dim // 2
    freqs = torch.exp(-math.log(10000) * torch.arange(0, half, device=device) / max(half, 1))
    args = t.float().unsqueeze(1) * freqs.unsqueeze(0)
    emb = torch.cat([torch.sin(args), torch.cos(args)], dim=1)
    if dim % 2 == 1:
        emb = F.pad(emb, (0, 1))
    return emb

# -----------------------------
# Attention Mechanisms
# -----------------------------
class SpatialAttention(nn.Module):
    """ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶ - GPUå†…å­˜ä¼˜åŒ–ç‰ˆæœ¬"""
    def __init__(self, dim):
        super().__init__()
        self.query = nn.Conv2d(dim, dim//16, 1)  # å‡å°‘é€šé“æ•°
        self.key = nn.Conv2d(dim, dim//16, 1)
        self.value = nn.Conv2d(dim, dim, 1)
        self.gamma = nn.Parameter(torch.zeros(1))
    
    def forward(self, x):
        B, C, H, W = x.shape
        
        # å¦‚æœç‰¹å¾å›¾å¤ªå¤§ï¼Œä½¿ç”¨å±€éƒ¨æ³¨æ„åŠ›
        if H * W > 32 * 32:  # å¦‚æœè¶…è¿‡32x32ï¼Œä½¿ç”¨å±€éƒ¨æ³¨æ„åŠ›
            return self._local_attention(x)
        
        Q = self.query(x).view(B, -1, H*W)
        K = self.key(x).view(B, -1, H*W)
        V = self.value(x).view(B, -1, H*W)
        
        attention = torch.bmm(Q.transpose(1,2), K)
        attention = F.softmax(attention, dim=-1)
        
        out = torch.bmm(V, attention.transpose(1,2))
        out = out.view(B, C, H, W)
        
        return self.gamma * out + x
    
    def _local_attention(self, x):
        """å±€éƒ¨æ³¨æ„åŠ›ï¼Œå‡å°‘è®¡ç®—é‡"""
        B, C, H, W = x.shape
        
        # ä½¿ç”¨3x3å·ç§¯æ¨¡æ‹Ÿå±€éƒ¨æ³¨æ„åŠ›
        local_conv = nn.Conv2d(C, C, 3, padding=1, groups=C//8).to(x.device)
        out = local_conv(x)
        
        return self.gamma * out + x

class ChannelAttention(nn.Module):
    """é€šé“æ³¨æ„åŠ›æœºåˆ¶"""
    def __init__(self, dim, reduction=16):
        super().__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)
        
        self.fc = nn.Sequential(
            nn.Conv2d(dim, dim//reduction, 1),
            nn.ReLU(),
            nn.Conv2d(dim//reduction, dim, 1)
        )
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x):
        avg_out = self.fc(self.avg_pool(x))
        max_out = self.fc(self.max_pool(x))
        attention = self.sigmoid(avg_out + max_out)
        return x * attention

class MultiHeadAttention(nn.Module):
    """å¤šå¤´è‡ªæ³¨æ„åŠ›"""
    def __init__(self, dim, num_heads=8):
        super().__init__()
        self.num_heads = num_heads
        self.dim = dim
        self.head_dim = dim // num_heads
        
        self.query = nn.Conv2d(dim, dim, 1)
        self.key = nn.Conv2d(dim, dim, 1)
        self.value = nn.Conv2d(dim, dim, 1)
        self.out = nn.Conv2d(dim, dim, 1)
        
    def forward(self, x):
        B, C, H, W = x.shape
        
        Q = self.query(x).view(B, self.num_heads, self.head_dim, H*W)
        K = self.key(x).view(B, self.num_heads, self.head_dim, H*W)
        V = self.value(x).view(B, self.num_heads, self.head_dim, H*W)
        
        # è®¡ç®—æ³¨æ„åŠ›
        attention = torch.matmul(Q.transpose(-2, -1), K) / math.sqrt(self.head_dim)
        attention = F.softmax(attention, dim=-1)
        
        out = torch.matmul(V, attention.transpose(-2, -1))
        out = out.view(B, C, H, W)
        
        return self.out(out) + x

# -----------------------------
# Enhanced UNet with Attention
# -----------------------------
class EnhancedResidualBlock(nn.Module):
    """å¢å¼ºçš„æ®‹å·®å—ï¼Œå®Œæ•´ç‰ˆæœ¬"""
    def __init__(self, in_ch, out_ch, time_dim, action_dim):
        super().__init__()
        
        # ä¸»åˆ†æ”¯
        self.norm1 = nn.GroupNorm(8, in_ch)
        self.act1 = nn.SiLU()
        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)
        
        # å®Œæ•´çš„æ³¨æ„åŠ›æœºåˆ¶
        self.spatial_attn = SpatialAttention(out_ch)
        self.channel_attn = ChannelAttention(out_ch)
        
        self.norm2 = nn.GroupNorm(8, out_ch)
        self.act2 = nn.SiLU()
        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)
        
        # æ¡ä»¶åµŒå…¥
        self.time_fc = nn.Linear(time_dim, out_ch)
        self.action_fc = nn.Linear(action_dim, out_ch)
        
        # è·³è·ƒè¿æ¥
        self.skip = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()
        
    def forward(self, x, t_emb, action_emb):
        h = self.conv1(self.act1(self.norm1(x)))
        
        # åº”ç”¨å®Œæ•´çš„æ³¨æ„åŠ›æœºåˆ¶
        h = self.spatial_attn(h)
        h = self.channel_attn(h)
        
        # æ·»åŠ æ¡ä»¶
        t = self.time_fc(t_emb).unsqueeze(-1).unsqueeze(-1)
        a = self.action_fc(action_emb).unsqueeze(-1).unsqueeze(-1)
        h = h + t + a
        
        h = self.conv2(self.act2(self.norm2(h)))
        
        return h + self.skip(x)

class EnhancedDown(nn.Module):
    """å¢å¼ºçš„ä¸‹é‡‡æ ·å—"""
    def __init__(self, in_ch, out_ch, time_dim, action_dim):
        super().__init__()
        self.block1 = EnhancedResidualBlock(in_ch, out_ch, time_dim, action_dim)
        self.block2 = EnhancedResidualBlock(out_ch, out_ch, time_dim, action_dim)
        self.pool = nn.Conv2d(out_ch, out_ch, 3, stride=2, padding=1)
        
    def forward(self, x, t_emb, action_emb):
        x = self.block1(x, t_emb, action_emb)
        x = self.block2(x, t_emb, action_emb)
        skip = x
        x = self.pool(x)
        return x, skip

class EnhancedUp(nn.Module):
    """å¢å¼ºçš„ä¸Šé‡‡æ ·å—"""
    def __init__(self, in_ch, skip_ch, out_ch, time_dim, action_dim):
        super().__init__()
        self.up = nn.ConvTranspose2d(in_ch, out_ch, 4, stride=2, padding=1)
        self.block1 = EnhancedResidualBlock(out_ch + skip_ch, out_ch, time_dim, action_dim)
        self.block2 = EnhancedResidualBlock(out_ch, out_ch, time_dim, action_dim)
        
    def forward(self, x, skip, t_emb, action_emb):
        x = self.up(x)
        x = torch.cat([x, skip], dim=1)
        x = self.block1(x, t_emb, action_emb)
        x = self.block2(x, t_emb, action_emb)
        return x

class EnhancedUNet(nn.Module):
    """å¢å¼ºçš„UNetï¼Œæ”¯æŒåŠ¨ä½œæ¡ä»¶ç”Ÿæˆ"""
    def __init__(self, in_ch=3, base_ch=128, time_dim=256, num_actions=256):
        super().__init__()
        self.time_dim = time_dim
        self.action_dim = 64
        
        # åŠ¨ä½œåµŒå…¥ (num_actions, action_dim)
        self.action_emb = nn.Embedding(num_actions, self.action_dim) 
        # 
        
        # åŠ¨ä½œåµŒå…¥ MLP
        self.action_mlp = nn.Sequential(
            nn.Linear(self.action_dim, self.action_dim * 4),
            nn.SiLU(),
            nn.Linear(self.action_dim * 4, self.action_dim),
        )
        
        # æ—¶é—´åµŒå…¥
        self.time_mlp = nn.Sequential(
            nn.Linear(time_dim, time_dim * 4),
            nn.SiLU(),
            nn.Linear(time_dim * 4, time_dim),
        )
        
        # ç¼–ç å™¨ - GPUå†…å­˜ä¼˜åŒ–ç‰ˆæœ¬
        self.in_conv = nn.Conv2d(in_ch, base_ch, 3, padding=1)
        self.down1 = EnhancedDown(base_ch, base_ch*2, time_dim, self.action_dim)
        self.down2 = EnhancedDown(base_ch*2, base_ch*4, time_dim, self.action_dim)
        
        # ç“¶é¢ˆå±‚ - GPUå†…å­˜ä¼˜åŒ–ç‰ˆæœ¬
        self.bot1 = EnhancedResidualBlock(base_ch*4, base_ch*4, time_dim, self.action_dim)
        self.bot2 = EnhancedResidualBlock(base_ch*4, base_ch*4, time_dim, self.action_dim)
        
        # è§£ç å™¨ - GPUå†…å­˜ä¼˜åŒ–ç‰ˆæœ¬
        self.up1 = EnhancedUp(base_ch*4, base_ch*4, base_ch*2, time_dim, self.action_dim)
        self.up2 = EnhancedUp(base_ch*2, base_ch*2, base_ch, time_dim, self.action_dim)
        
        # è¾“å‡ºå±‚
        self.out_norm = nn.GroupNorm(8, base_ch)
        self.out_conv = nn.Conv2d(base_ch, in_ch, 3, padding=1)
        
    def forward(self, x, t, action_labels):
        # æ—¶é—´åµŒå…¥
        t_emb = sinusoidal_time_embedding(t, self.time_dim)
        t_emb = self.time_mlp(t_emb)
        
        # åŠ¨ä½œåµŒå…¥
        action_emb = self.action_emb(action_labels)
        action_emb = self.action_mlp(action_emb)
        
        # ç¼–ç å™¨
        x = self.in_conv(x)
        x1, s1 = self.down1(x, t_emb, action_emb)
        x2, s2 = self.down2(x1, t_emb, action_emb)
        
        # ç“¶é¢ˆå±‚
        x = self.bot1(x2, t_emb, action_emb)
        x = self.bot2(x, t_emb, action_emb)
        
        # è§£ç å™¨
        x = self.up1(x, s2, t_emb, action_emb)
        x = self.up2(x, s1, t_emb, action_emb)
        
        # è¾“å‡º
        x = F.silu(self.out_norm(x))
        return self.out_conv(x)

# -----------------------------
# Enhanced Flow Matching
# -----------------------------
class EnhancedFlowMatching:
    """å¢å¼ºçš„Flow Matchingï¼Œæ”¯æŒåŠ¨ä½œæ¡ä»¶"""
    def __init__(self, device):
        self.device = device

    def make_batch(self, x0, action_labels):
        b = x0.size(0)
        t = torch.rand(b, device=self.device)
        eps = torch.randn_like(x0)
        x_t = (1.0 - t.view(-1,1,1,1)) * x0 + t.view(-1,1,1,1) * eps
        v_target = eps - x0
        return x_t, t, v_target, action_labels

    @torch.no_grad()
    def sample(self, model, shape, action_labels=None, steps=100, device="cpu"):
        model.eval()
        x = torch.randn(shape, device=device)
        dt = -1.0 / steps
        
        for k in range(steps):
            t_scalar = 1.0 + dt * k
            t = torch.full((shape[0],), t_scalar, device=device, dtype=torch.float32).clamp(0,1)
            v = model(x, t, action_labels)
            x = x + dt * v
            
        return x

# -----------------------------
# Sequence Generation
# -----------------------------
class SequenceGenerator:
    """åºåˆ—ç”Ÿæˆå™¨ï¼Œæ ¹æ®åŠ¨ä½œåºåˆ—ç”Ÿæˆæ¸¸æˆçŠ¶æ€"""
    def __init__(self, model, flow_matching, device):
        self.model = model
        self.flow_matching = flow_matching
        self.device = device
        
    def generate_sequence(self, action_sequence: List[int], initial_noise=None):
        """æ ¹æ®åŠ¨ä½œåºåˆ—ç”Ÿæˆæ¸¸æˆçŠ¶æ€åºåˆ—"""
        self.model.eval()
        
        sequence_length = len(action_sequence)
        generated_frames = []
        
        # åˆå§‹å™ªå£°
        if initial_noise is None:
            current_state = torch.randn(1, cfg.in_ch, cfg.image_size, cfg.image_size, device=self.device)
        else:
            current_state = initial_noise
            
        with torch.no_grad():
            for i, action in enumerate(action_sequence):
                # åˆ›å»ºåŠ¨ä½œæ ‡ç­¾
                action_label = torch.tensor([action], device=self.device)
                
                # ç”Ÿæˆå½“å‰å¸§
                frame = self.flow_matching.sample(
                    self.model,
                    (1, cfg.in_ch, cfg.image_size, cfg.image_size),
                    action_labels=action_label,
                    steps=cfg.ode_steps,
                    device=self.device
                )
                
                generated_frames.append(frame)
                
                # æ›´æ–°çŠ¶æ€ï¼ˆç®€å•çš„çŠ¶æ€ä¼ é€’ï¼‰
                current_state = frame
                
        return generated_frames

# -----------------------------
# Enhanced Training
# -----------------------------
def train_enhanced():
    """å¢å¼ºçš„è®­ç»ƒå‡½æ•°"""
    device = torch.device(cfg.device)
    
    # åŠ è½½æ•°æ®
    dataset = MarioDataset(cfg.data_path, cfg.image_size)
    
    # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦ä¸ºç©º
    if len(dataset) == 0:
        print("âŒ æ•°æ®é›†ä¸ºç©ºï¼Œæ— æ³•è®­ç»ƒ")
        print("ğŸ’¡ è¯·æ£€æŸ¥:")
        print("   1. æ•°æ®è·¯å¾„æ˜¯å¦æ­£ç¡®")
        print("   2. PNGæ–‡ä»¶æ˜¯å¦å­˜åœ¨")
        print("   3. æ–‡ä»¶åæ ¼å¼æ˜¯å¦æ­£ç¡®")
        return
    
    dataloader = DataLoader(
        dataset, 
        batch_size=cfg.batch_size, 
        shuffle=True,  # æ˜¯å¦åº”è¯¥æ‰“ä¹±æ•°æ®
        num_workers=cfg.num_workers, 
        pin_memory=cfg.pin_memory
    )
    
    # åˆ›å»ºæ¨¡å‹
    model = EnhancedUNet(
        in_ch=cfg.in_ch,
        base_ch=cfg.base_ch,
        time_dim=256,
        num_actions=cfg.num_actions
    ).to(device)
    
    # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=0.01)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg.epochs)
    
    # æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = torch.cuda.amp.GradScaler() if cfg.mixed_precision and device.type == 'cuda' else None
    
    # Flow Matching
    fm = EnhancedFlowMatching(device)
    
    # æŸå¤±è·Ÿè¸ª
    losses = []
    
    print(f"ğŸš€ å¼€å§‹å¢å¼ºè®­ç»ƒ")
    print(f"   æ•°æ®é‡: {len(dataset)}")
    print(f"   æ‰¹æ¬¡å¤§å°: {cfg.batch_size}")
    print(f"   è®­ç»ƒè½®æ•°: {cfg.epochs}")
    print(f"   è®¾å¤‡: {device}")
    
    # GPUä¿¡æ¯
    if device.type == 'cuda':
        print(f"   GPU: {torch.cuda.get_device_name(0)}")
        print(f"   GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        print(f"   æ··åˆç²¾åº¦: {'å¯ç”¨' if scaler is not None else 'ç¦ç”¨'}")
        
        # æ¸…ç†GPUå†…å­˜
        torch.cuda.empty_cache()
        print(f"   GPUå†…å­˜å·²æ¸…ç†")
    else:
        print(f"   CPUè®­ç»ƒæ¨¡å¼")
    
    model.train()
    
    for epoch in range(1, cfg.epochs + 1):
        epoch_loss = 0
        num_batches = 0
        
        for batch_idx, (x0, action_labels) in enumerate(dataloader):
            x0 = x0.to(device)
            action_labels = action_labels.to(device)
            
            # Flow Matchingè®­ç»ƒ
            x_t, t, v_target, action_labels = fm.make_batch(x0, action_labels)
            
            # æ··åˆç²¾åº¦è®­ç»ƒ
            if scaler is not None:
                with torch.amp.autocast('cuda'):
                    v_pred = model(x_t, t, action_labels)
                    loss = F.mse_loss(v_pred, v_target)
                
                optimizer.zero_grad()
                scaler.scale(loss).backward()
                scaler.unscale_(optimizer)
                nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)
                scaler.step(optimizer)
                scaler.update()
            else:
                v_pred = model(x_t, t, action_labels)
                loss = F.mse_loss(v_pred, v_target)
                
                optimizer.zero_grad()
                loss.backward()
                nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)
                optimizer.step()
            
            epoch_loss += loss.item()
            num_batches += 1
            
            if batch_idx % 50 == 0:
                print(f"[Epoch {epoch}] Batch {batch_idx} Loss: {loss.item():.6f}")
        
        avg_loss = epoch_loss / num_batches
        losses.append(avg_loss)
        
        print(f"Epoch {epoch} completed. Avg Loss: {avg_loss:.6f}")
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step()
        
        # æ¸…ç†GPUå†…å­˜
        if device.type == 'cuda':
            torch.cuda.empty_cache()
        
        # æ¯20ä¸ªepochç”Ÿæˆæ ·æœ¬
        if epoch % 20 == 0:
            with torch.no_grad():
                # ç”Ÿæˆå•ä¸ªæ ·æœ¬
                for action in [0, 4, 16, 20]:  # ä¸åŒåŠ¨ä½œ
                    action_label = torch.tensor([action], device=device)
                    sample = fm.sample(
                        model,
                        (1, cfg.in_ch, cfg.image_size, cfg.image_size),
                        action_labels=action_label,
                        steps=cfg.ode_steps,
                        device=device
                    )
                    sample = (sample.clamp(-1, 1) + 1) / 2.0
                    save_image(sample, os.path.join(cfg.out_dir, f"sample_epoch{epoch}_action{action}.png"))
                
                # ç”ŸæˆåŠ¨ä½œåºåˆ—
                sequence_generator = SequenceGenerator(model, fm, device)
                action_sequence = [4, 4, 32, 32, 4, 4]  # å³å³å·¦å·¦å³å³
                frames = sequence_generator.generate_sequence(action_sequence)
                
                # ä¿å­˜åºåˆ—ä¸ºGIF
                gif_frames = []
                for i, frame in enumerate(frames):
                    frame = (frame.clamp(-1, 1) + 1) / 2.0
                    
                    # è½¬æ¢ä¸ºPILå›¾åƒ
                    frame_np = frame.squeeze().cpu().numpy().transpose(1, 2, 0)
                    frame_pil = Image.fromarray((frame_np * 255).astype(np.uint8))
                    gif_frames.append(frame_pil)
                    
                    # åŒæ—¶ä¿å­˜å•å¼ å›¾ç‰‡ï¼ˆå¯é€‰ï¼‰
                    save_image(frame, os.path.join(cfg.out_dir, f"sequence_epoch{epoch}_frame{i}.png"))
                
                # ä¿å­˜GIF
                gif_path = os.path.join(cfg.out_dir, f"sequence_epoch{epoch}.gif")
                gif_frames[0].save(
                    gif_path,
                    save_all=True,
                    append_images=gif_frames[1:],
                    duration=500,  # æ¯å¸§500ms
                    loop=0  # æ— é™å¾ªç¯
                )
                print(f"ğŸ¬ åŠ¨ä½œåºåˆ—GIFå·²ä¿å­˜: {gif_path}")
    
    # ä¿å­˜æ¨¡å‹
    torch.save(model.state_dict(), os.path.join(cfg.out_dir, "enhanced_mario_model.pt"))
    
    # ç»˜åˆ¶æŸå¤±æ›²çº¿
    plt.figure(figsize=(10, 6))
    plt.plot(losses)
    plt.title('Training Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.savefig(os.path.join(cfg.out_dir, 'loss_curve.png'))
    plt.close()
    
    print(f"ğŸ‰ è®­ç»ƒå®Œæˆ! æ¨¡å‹ä¿å­˜åœ¨: {cfg.out_dir}")

# -----------------------------
# Test Sequence Generation
# -----------------------------
def test_sequence_generation():
    """æµ‹è¯•åºåˆ—ç”ŸæˆåŠŸèƒ½"""
    device = torch.device(cfg.device)
    
    # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    model = EnhancedUNet(
        in_ch=cfg.in_ch,
        base_ch=cfg.base_ch,
        time_dim=256,
        num_actions=cfg.num_actions
    ).to(device)
    
    model_path = os.path.join(cfg.out_dir, "enhanced_mario_model.pt")
    if os.path.exists(model_path):
        model.load_state_dict(torch.load(model_path, map_location=device))
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    else:
        print("âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹")
        return
    
    # åˆ›å»ºåºåˆ—ç”Ÿæˆå™¨
    fm = EnhancedFlowMatching(device)
    sequence_generator = SequenceGenerator(model, fm, device)
    
    # æµ‹è¯•ä¸åŒçš„åŠ¨ä½œåºåˆ—
    test_sequences = [
        [4, 4, 4, 4, 4, 4],      # è¿ç»­å‘å³
        [32, 32, 32, 32, 32, 32], # è¿ç»­å‘å·¦
        [4, 128, 4, 128, 4, 128], # å³è·³å³è·³å³è·³
        [4, 4, 32, 32, 4, 4],     # å³å³å·¦å·¦å³å³
        [20, 20, 20, 20, 20, 20], # è·‘æ­¥å‘å³
    ]
    
    for seq_idx, action_sequence in enumerate(test_sequences):
        print(f"ğŸ® ç”ŸæˆåŠ¨ä½œåºåˆ— {seq_idx + 1}: {action_sequence}")
        
        frames = sequence_generator.generate_sequence(action_sequence)
        
        # ä¿å­˜åºåˆ—ä¸ºGIF
        gif_frames = []
        for i, frame in enumerate(frames):
            frame = (frame.clamp(-1, 1) + 1) / 2.0
            
            # è½¬æ¢ä¸ºPILå›¾åƒ
            frame_np = frame.squeeze().cpu().numpy().transpose(1, 2, 0)
            frame_pil = Image.fromarray((frame_np * 255).astype(np.uint8))
            gif_frames.append(frame_pil)
            
            # åŒæ—¶ä¿å­˜å•å¼ å›¾ç‰‡
            save_image(frame, os.path.join(cfg.out_dir, f"test_sequence_{seq_idx}_frame_{i}.png"))
        
        # ä¿å­˜GIF
        gif_path = os.path.join(cfg.out_dir, f"test_sequence_{seq_idx}.gif")
        gif_frames[0].save(
            gif_path,
            save_all=True,
            append_images=gif_frames[1:],
            duration=500,  # æ¯å¸§500ms
            loop=0  # æ— é™å¾ªç¯
        )
        
        print(f"ğŸ¬ åŠ¨ä½œåºåˆ—GIFå·²ä¿å­˜: {gif_path}")
        print(f"ğŸ“ å•å¼ å›¾ç‰‡å·²ä¿å­˜åˆ°: test_sequence_{seq_idx}_frame_*.png")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        if sys.argv[1] == "train":
            print("ğŸš€ å¼€å§‹è®­ç»ƒå¢å¼ºç‰ˆMario Flow Matchingæ¨¡å‹")
            train_enhanced()
        elif sys.argv[1] == "test":
            print("ğŸ§ª æµ‹è¯•åºåˆ—ç”ŸæˆåŠŸèƒ½")
            test_sequence_generation()
        else:
            print("âŒ æœªçŸ¥å‚æ•°")
            print("ğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
            print("   python flow_matching_mario_enhanced.py train  # è®­ç»ƒæ¨¡å‹")
            print("   python flow_matching_mario_enhanced.py test   # æµ‹è¯•åºåˆ—ç”Ÿæˆ")
    else:
        print("ğŸš€ å¼€å§‹è®­ç»ƒå¢å¼ºç‰ˆMario Flow Matchingæ¨¡å‹")
        train_enhanced()
