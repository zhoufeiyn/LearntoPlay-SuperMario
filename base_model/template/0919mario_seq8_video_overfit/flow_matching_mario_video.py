# Flow Matching for Super Mario Bros Video Generation
# åŸºäºflow_matching_mario_train.pyï¼Œä¸“é—¨ç”¨äºç”Ÿæˆè¶…çº§ç›ä¸½è§†é¢‘çš„Flow Matchingæ¨¡å‹

import math
import os
import time
import cv2
import numpy as np
from dataclasses import dataclass
from typing import List, Tuple, Optional
import re

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import torchvision
from torchvision import transforms
from torchvision.utils import save_image

import matplotlib.pyplot as plt
from PIL import Image
import imageio

# -----------------------------
# Config
# -----------------------------
@dataclass
class Config:
    # è§†é¢‘å‚æ•°
    video_height: int = 240      # åŸå§‹æ¸¸æˆåˆ†è¾¨ç‡
    video_width: int = 256       # è°ƒæ•´ä¸º256x240
    video_channels: int = 3      # RGB
    sequence_length: int = 8    # è§†é¢‘åºåˆ—é•¿åº¦ï¼ˆå¸§æ•°ï¼‰- 2ç§’@30FPS
    
    # æ¨¡å‹å‚æ•°
    base_ch: int = 64            # åŸºç¡€é€šé“æ•°
    time_dim: int = 256          # æ—¶é—´åµŒå…¥ç»´åº¦
    
    # è®­ç»ƒå‚æ•°
    batch_size: int = 1          # æ›´å°æ‰¹æ¬¡ä»¥é€‚åº”æ›´é•¿åºåˆ—çš„GPUå†…å­˜éœ€æ±‚
    epochs: int = 5
    lr: float = 1e-4
    num_samples: int = 4
    
    grad_clip: float = 1.0
    ode_steps: int = 10 # åŸæ¥æ˜¯100
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    
    # æ•°æ®å‚æ•°
    num_workers: int = 4
    pin_memory: bool = True
    out_dir: str = "./output/mario_video"
    data_path: str = "./data_video/mario"
    
    # è§†é¢‘ç”Ÿæˆå‚æ•°
    fps: int = 30                # ç”Ÿæˆè§†é¢‘çš„å¸§ç‡
    video_duration: int = 5      # ç”Ÿæˆè§†é¢‘çš„æ—¶é•¿ï¼ˆç§’ï¼‰

cfg = Config()
os.makedirs(cfg.out_dir, exist_ok=True)

# -----------------------------
# Video Dataset
# -----------------------------
class MarioVideoDataset(Dataset):
    """è¶…çº§ç›ä¸½è§†é¢‘æ•°æ®é›†"""
    def __init__(self, data_path: str, sequence_length: int = 16):
        self.data_path = data_path
        self.sequence_length = sequence_length
        self.video_files = []
        self.video_sequences = []
        
        # åŠ è½½è§†é¢‘æ–‡ä»¶
        self._load_videos()
        
        # æ•°æ®å˜æ¢
        self.transform = transforms.Compose([
            transforms.Resize((cfg.video_height, cfg.video_width)),
            transforms.ToTensor(),
            transforms.Normalize(0.5, 0.5),  # [-1, 1]
        ])
    
    def _load_videos(self):
        """åŠ è½½æ‰€æœ‰AVIè§†é¢‘æ–‡ä»¶"""
        print(f"ğŸ” æ­£åœ¨æ‰«æè§†é¢‘æ•°æ®è·¯å¾„: {self.data_path}")
        
        if not os.path.exists(self.data_path):
            print(f"âŒ æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {self.data_path}")
            return
        
        # æŸ¥æ‰¾æ‰€æœ‰AVIæ–‡ä»¶
        for file in os.listdir(self.data_path):
            if file.endswith('.avi'):
                video_path = os.path.join(self.data_path, file)
                self.video_files.append(video_path)
                print(f"ğŸ“¹ æ‰¾åˆ°è§†é¢‘æ–‡ä»¶: {file}")
        
        print(f"âœ… æ€»å…±æ‰¾åˆ° {len(self.video_files)} ä¸ªè§†é¢‘æ–‡ä»¶")
        
        # ä»æ¯ä¸ªè§†é¢‘ä¸­æå–åºåˆ—
        self._extract_sequences()
    
    def _extract_sequences(self):
        """ä»è§†é¢‘ä¸­æå–å¸§åºåˆ—"""
        print("ğŸ¬ æ­£åœ¨ä»è§†é¢‘ä¸­æå–å¸§åºåˆ—...")
        
        for video_path in self.video_files:
            cap = cv2.VideoCapture(video_path)
            
            if not cap.isOpened():
                print(f"âš ï¸ æ— æ³•æ‰“å¼€è§†é¢‘: {video_path}")
                continue
            
            # è·å–è§†é¢‘ä¿¡æ¯
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            
            print(f"ğŸ“Š è§†é¢‘ä¿¡æ¯: {os.path.basename(video_path)}")
            print(f"   æ€»å¸§æ•°: {total_frames}")
            print(f"   å¸§ç‡: {fps:.2f} FPS")
            
            # æå–åºåˆ—
            frame_count = 0
            frames = []
            
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                # è½¬æ¢é¢œè‰²ç©ºé—´ BGR -> RGB
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                frames.append(frame)
                frame_count += 1
            
            cap.release()
            
            # å°†å¸§åˆ†ç»„ä¸ºåºåˆ—
            if len(frames) >= self.sequence_length:
                for i in range(0, len(frames) - self.sequence_length + 1, self.sequence_length // 2):
                    sequence = frames[i:i + self.sequence_length]
                    if len(sequence) == self.sequence_length:
                        self.video_sequences.append(sequence)
            
            print(f"   æå–äº† {len(frames)} å¸§ï¼Œç”Ÿæˆäº† {len([s for s in self.video_sequences if len(s) == self.sequence_length])} ä¸ªåºåˆ—")
        
        print(f"âœ… æ€»å…±æå–äº† {len(self.video_sequences)} ä¸ªè§†é¢‘åºåˆ—")
    
    def __len__(self):
        return len(self.video_sequences)
    def __getitem__(self, idx):
        # åªä½¿ç”¨ç¬¬60ä¸ªåºåˆ—è¿›è¡Œè®­ç»ƒ
        sequence = self.video_sequences[60]
        
        # è½¬æ¢å¸§ä¸ºtensor
        tensor_sequence = []
        for frame in sequence:
            # è½¬æ¢ä¸ºPILå›¾åƒ
            frame_pil = Image.fromarray(frame)
            # åº”ç”¨å˜æ¢
            frame_tensor = self.transform(frame_pil)
            tensor_sequence.append(frame_tensor)
        
        # å †å ä¸ºè§†é¢‘tensor (T, C, H, W)
        video_tensor = torch.stack(tensor_sequence)
        
        return video_tensor

# -----------------------------
# Time Embedding
# -----------------------------
def sinusoidal_time_embedding(t, dim):
    """æ—¶é—´åµŒå…¥"""
    device = t.device
    half = dim // 2
    freqs = torch.exp(-math.log(10000) * torch.arange(0, half, device=device) / max(half, 1))
    args = t.float().unsqueeze(1) * freqs.unsqueeze(0)
    emb = torch.cat([torch.sin(args), torch.cos(args)], dim=1)
    if dim % 2 == 1:
        emb = F.pad(emb, (0, 1))
    return emb

# -----------------------------
# 3D Convolution Blocks for Video
# -----------------------------
class Conv3DBlock(nn.Module):
    """3Då·ç§¯å—"""
    def __init__(self, in_ch, out_ch, time_dim, kernel_size=3, stride=1, padding=1):
        super().__init__()
        self.conv = nn.Conv3d(in_ch, out_ch, kernel_size, stride=stride, padding=padding)
        self.norm = nn.GroupNorm(min(8, out_ch), out_ch)
        self.act = nn.SiLU()
        self.time_fc = nn.Linear(time_dim, out_ch)
        
    def forward(self, x, t_emb):
        h = self.conv(x)
        h = self.norm(h)
        h = self.act(h)
        
        # æ·»åŠ æ—¶é—´æ¡ä»¶
        t = self.time_fc(t_emb).unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)
        h = h + t
        
        return h

class Residual3DBlock(nn.Module):
    """3Dæ®‹å·®å—"""
    def __init__(self, in_ch, out_ch, time_dim):
        super().__init__()
        self.conv1 = nn.Conv3d(in_ch, out_ch, 3, padding=1)
        self.norm1 = nn.GroupNorm(min(8, out_ch), out_ch)
        self.act1 = nn.SiLU()
        
        self.conv2 = nn.Conv3d(out_ch, out_ch, 3, padding=1)
        self.norm2 = nn.GroupNorm(min(8, out_ch), out_ch)
        self.act2 = nn.SiLU()
        
        self.time_fc = nn.Linear(time_dim, out_ch)
        self.skip = nn.Conv3d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()
        
    def forward(self, x, t_emb):
        h = self.conv1(x)
        h = self.norm1(h)
        h = self.act1(h)
        
        # æ·»åŠ æ—¶é—´æ¡ä»¶
        t = self.time_fc(t_emb).unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)
        h = h + t
        
        h = self.conv2(h)
        h = self.norm2(h)
        h = self.act2(h)
        
        return h + self.skip(x)

class Down3D(nn.Module):
    """3Dä¸‹é‡‡æ ·"""
    def __init__(self, in_ch, out_ch, time_dim):
        super().__init__()
        self.block1 = Residual3DBlock(in_ch, out_ch, time_dim)
        self.block2 = Residual3DBlock(out_ch, out_ch, time_dim)
        self.pool = nn.Conv3d(out_ch, out_ch, (1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
        
    def forward(self, x, t_emb):
        x = self.block1(x, t_emb)
        x = self.block2(x, t_emb)
        skip = x
        x = self.pool(x)
        return x, skip

class Up3D(nn.Module):
    """3Dä¸Šé‡‡æ ·"""
    def __init__(self, in_ch, skip_ch, out_ch, time_dim):
        super().__init__()
        self.up = nn.ConvTranspose3d(in_ch, out_ch, (1, 4, 4), stride=(1, 2, 2), padding=(0, 1, 1))
        self.block1 = Residual3DBlock(out_ch + skip_ch, out_ch, time_dim)
        self.block2 = Residual3DBlock(out_ch, out_ch, time_dim)
        
    def forward(self, x, skip, t_emb):
        x = self.up(x)
        x = torch.cat([x, skip], dim=1)
        x = self.block1(x, t_emb)
        x = self.block2(x, t_emb)
        return x

# -----------------------------
# 3D UNet for Video Generation
# -----------------------------
class VideoUNet(nn.Module):
    """ç”¨äºè§†é¢‘ç”Ÿæˆçš„3D UNet"""
    def __init__(self, in_ch=3, base_ch=64, time_dim=256, sequence_length=16):
        super().__init__()
        self.time_dim = time_dim
        self.sequence_length = sequence_length
        
        # æ—¶é—´åµŒå…¥MLP
        self.time_mlp = nn.Sequential(
            nn.Linear(time_dim, time_dim * 4),
            nn.SiLU(),
            nn.Linear(time_dim * 4, time_dim),
        )
        
        # ç¼–ç å™¨
        self.in_conv = nn.Conv3d(in_ch, base_ch, 3, padding=1)
        self.down1 = Down3D(base_ch, base_ch*2, time_dim)
        self.down2 = Down3D(base_ch*2, base_ch*4, time_dim)
        
        # ç“¶é¢ˆå±‚
        self.bot1 = Residual3DBlock(base_ch*4, base_ch*4, time_dim)
        self.bot2 = Residual3DBlock(base_ch*4, base_ch*4, time_dim)
        
        # è§£ç å™¨
        self.up1 = Up3D(base_ch*4, base_ch*4, base_ch*2, time_dim)
        self.up2 = Up3D(base_ch*2, base_ch*2, base_ch, time_dim)
        
        # è¾“å‡ºå±‚
        self.out_norm = nn.GroupNorm(min(8, base_ch), base_ch)
        self.out_conv = nn.Conv3d(base_ch, in_ch, 3, padding=1)
        
    def forward(self, x, t):
        # x: (B, C, T, H, W)
        t_emb = sinusoidal_time_embedding(t, self.time_dim)
        t_emb = self.time_mlp(t_emb)
        
        # ç¼–ç å™¨
        x = self.in_conv(x)
        x1, s1 = self.down1(x, t_emb)
        x2, s2 = self.down2(x1, t_emb)
        
        # ç“¶é¢ˆå±‚
        x = self.bot1(x2, t_emb)
        x = self.bot2(x, t_emb)
        
        # è§£ç å™¨
        x = self.up1(x, s2, t_emb)
        x = self.up2(x, s1, t_emb)
        
        # è¾“å‡º
        x = F.silu(self.out_norm(x))
        return self.out_conv(x)

# -----------------------------
# Video Flow Matching
# -----------------------------
class VideoFlowMatching:
    """è§†é¢‘Flow Matching"""
    def __init__(self, device):
        self.device = device

    def make_batch(self, x0):
        """åˆ›å»ºè®­ç»ƒæ‰¹æ¬¡"""
        b = x0.size(0)
        t = torch.rand(b, device=self.device)
        eps = torch.randn_like(x0)
        x_t = (1.0 - t.view(-1,1,1,1,1)) * x0 + t.view(-1,1,1,1,1) * eps
        v_target = eps - x0
        return x_t, t, v_target

    @torch.no_grad()
    def sample(self, model, shape, steps=100, device="cpu"):
        """é‡‡æ ·ç”Ÿæˆè§†é¢‘"""
        model.eval()
        x = torch.randn(shape, device=device)
        dt = -1.0 / steps
        
        for k in range(steps):
            t_scalar = 1.0 + dt * k
            t = torch.full((shape[0],), t_scalar, device=device, dtype=torch.float32).clamp(0,1)
            v = model(x, t)
            x = x + dt * v
            
        return x

# -----------------------------
# Video Generation Utilities
# -----------------------------
def save_video_tensor(video_tensor, save_path, fps=30):
    """ä¿å­˜è§†é¢‘tensorä¸ºè§†é¢‘æ–‡ä»¶"""
    # video_tensor: (T, C, H, W)
    video_tensor = (video_tensor.clamp(-1, 1) + 1) / 2.0  # [-1,1] -> [0,1]
    
    print(f"ğŸ” è§†é¢‘å¼ é‡å½¢çŠ¶: {video_tensor.shape}")
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    frames = []
    for t in range(video_tensor.size(0)):
        frame = video_tensor[t].cpu().numpy()
        print(f"ğŸ” å¸§ {t} åŸå§‹å½¢çŠ¶: {frame.shape}")
        
        # æ£€æŸ¥ç»´åº¦å¹¶å¤„ç†
        if frame.ndim == 3:
            # å¦‚æœæ˜¯ (C, H, W)ï¼Œè½¬æ¢ä¸º (H, W, C)
            if frame.shape[0] == 3:  # é€šé“åœ¨ç¬¬ä¸€ç»´
                frame = frame.transpose(1, 2, 0)
                print(f"ğŸ” å¸§ {t} è½¬æ¢åå½¢çŠ¶: {frame.shape}")
            elif frame.shape[2] == 3:  # é€šé“åœ¨ç¬¬ä¸‰ç»´ï¼Œå·²ç»æ˜¯ (H, W, C)
                print(f"ğŸ” å¸§ {t} å·²ç»æ˜¯æ­£ç¡®æ ¼å¼: {frame.shape}")
            else:
                print(f"âš ï¸ å¸§ {t} ç»´åº¦ä¸æ­£ç¡®: {frame.shape}")
                continue
        else:
            print(f"âš ï¸ å¸§ {t} ä¸æ˜¯3ç»´: {frame.shape}")
            continue
        
        # ç¡®ä¿æ•°æ®ç±»å‹å’ŒèŒƒå›´æ­£ç¡®
        frame = np.clip(frame, 0, 1)  # ç¡®ä¿åœ¨[0,1]èŒƒå›´å†…
        frame = (frame * 255).astype(np.uint8)
        
        # ç¡®ä¿æ˜¯RGBæ ¼å¼
        if frame.shape[2] == 3:
            frames.append(frame)
            print(f"âœ… å¸§ {t} æ·»åŠ æˆåŠŸ: {frame.shape}")
        else:
            print(f"âš ï¸ è·³è¿‡å¸§ {t}: é€šé“æ•°ä¸æ­£ç¡® {frame.shape}")
    
    if not frames:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„å¸§å¯ä»¥ä¿å­˜")
        return
    
    # ä¿å­˜ä¸ºGIF
    if save_path.endswith('.gif'):
        try:
            imageio.mimsave(save_path, frames, fps=fps)
            print(f"âœ… GIFä¿å­˜æˆåŠŸ: {save_path}")
        except Exception as e:
            print(f"âŒ GIFä¿å­˜å¤±è´¥: {e}")
            # å°è¯•ä¿å­˜ä¸ºMP4
            mp4_path = save_path.replace('.gif', '.mp4')
            save_video_tensor(video_tensor, mp4_path, fps)
    else:
        # ä¿å­˜ä¸ºMP4
        try:
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            height, width = frames[0].shape[:2]
            writer = cv2.VideoWriter(save_path, fourcc, fps, (width, height))
            
            for frame in frames:
                frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
                writer.write(frame_bgr)
            
            writer.release()
            print(f"âœ… MP4ä¿å­˜æˆåŠŸ: {save_path}")
        except Exception as e:
            print(f"âŒ MP4ä¿å­˜å¤±è´¥: {e}")

def create_video_grid(videos, save_path, fps=30):
    """åˆ›å»ºè§†é¢‘ç½‘æ ¼"""
    # videos: list of (T, C, H, W) tensors
    num_videos = len(videos)
    grid_size = int(math.ceil(math.sqrt(num_videos)))
    
    # è°ƒæ•´æ‰€æœ‰è§†é¢‘åˆ°ç›¸åŒå°ºå¯¸
    target_h, target_w = 64, 64  # ç½‘æ ¼ä¸­æ¯ä¸ªè§†é¢‘çš„å°ºå¯¸
    
    processed_videos = []
    for video in videos:
        video = (video.clamp(-1, 1) + 1) / 2.0
        # è°ƒæ•´å°ºå¯¸
        video_resized = F.interpolate(video, size=(target_h, target_w), mode='bilinear', align_corners=False)
        processed_videos.append(video_resized)
    
    # åˆ›å»ºç½‘æ ¼
    frames = []
    T = processed_videos[0].size(0)
    
    for t in range(T):
        frame_grid = []
        for i in range(grid_size):
            row = []
            for j in range(grid_size):
                idx = i * grid_size + j
                if idx < len(processed_videos):
                    frame = processed_videos[idx][t].cpu().numpy()
                    
                    # ç¡®ä¿ç»´åº¦æ­£ç¡®
                    if frame.ndim == 3:
                        frame = frame.transpose(1, 2, 0)
                    
                    # ç¡®ä¿æ•°æ®ç±»å‹å’ŒèŒƒå›´æ­£ç¡®
                    frame = np.clip(frame, 0, 1)
                    frame = (frame * 255).astype(np.uint8)
                else:
                    frame = np.zeros((target_h, target_w, 3), dtype=np.uint8)
                row.append(frame)
            frame_grid.append(np.concatenate(row, axis=1))
        
        grid_frame = np.concatenate(frame_grid, axis=0)
        frames.append(grid_frame)
    
    # ä¿å­˜ä¸ºGIF
    imageio.mimsave(save_path, frames, fps=fps)

# -----------------------------
# Training Function
# -----------------------------
def train_video_model():
    """è®­ç»ƒè§†é¢‘ç”Ÿæˆæ¨¡å‹"""
    device = torch.device(cfg.device)
    
    # åŠ è½½æ•°æ®
    dataset = MarioVideoDataset(cfg.data_path, cfg.sequence_length)
    
    if len(dataset) == 0:
        print("âŒ æ•°æ®é›†ä¸ºç©ºï¼Œæ— æ³•è®­ç»ƒ")
        return
    
    # dataloader = DataLoader(
    #     dataset,
    #     batch_size=cfg.batch_size,
    #     shuffle=True,
    #     num_workers=cfg.num_workers,
    #     pin_memory=cfg.pin_memory
    # )
    dataloader = DataLoader(
        dataset,
        batch_size=cfg.batch_size,
        shuffle=False,
        num_workers=cfg.num_workers,
        pin_memory=cfg.pin_memory
    )

    # åˆ›å»ºæ¨¡å‹
    model = VideoUNet(
        in_ch=cfg.video_channels,
        base_ch=cfg.base_ch,
        time_dim=cfg.time_dim,
        sequence_length=cfg.sequence_length
    ).to(device)
    
    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=0.01)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg.epochs)
    
    # Flow Matching
    fm = VideoFlowMatching(device)
    
    # æŸå¤±è·Ÿè¸ª
    losses = []
    
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒè§†é¢‘ç”Ÿæˆæ¨¡å‹")
    print(f"   æ•°æ®é‡: {len(dataset)}")
    print(f"   æ‰¹æ¬¡å¤§å°: {cfg.batch_size}")
    print(f"   è®­ç»ƒè½®æ•°: {cfg.epochs}")
    print(f"   è®¾å¤‡: {device}")
    print(f"   åºåˆ—é•¿åº¦: {cfg.sequence_length}")
    
    model.train()
    
    for epoch in range(1, cfg.epochs + 1):
        epoch_loss = 0
        num_batches = 0
        
        for batch_idx, video_batch in enumerate(dataloader):
            # video_batch: (B, T, C, H, W) -> (B, C, T, H, W)
            video_batch = video_batch.transpose(1, 2).to(device)
            
            # Flow Matchingè®­ç»ƒ
            x_t, t, v_target = fm.make_batch(video_batch)
            v_pred = model(x_t, t)
            loss = F.mse_loss(v_pred, v_target)
            
            optimizer.zero_grad()
            loss.backward()
            nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)
            optimizer.step()
            
            epoch_loss += loss.item()
            num_batches += 1
            
            if batch_idx % 10 == 0:
                print(f"[Epoch {epoch}] Batch {batch_idx} Loss: {loss.item():.6f}")
            # if batch_idx > 11:
            #     break
        
        avg_loss = epoch_loss / num_batches
        losses.append(avg_loss)
        
        print(f"Epoch {epoch} completed. Avg Loss: {avg_loss:.6f}")
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step()
        
        # # æ¯10ä¸ªepochç”Ÿæˆæ ·æœ¬
        if epoch % 1 == 0:
            with torch.no_grad():
                # ç”Ÿæˆå•ä¸ªè§†é¢‘
                sample_video = fm.sample(
                    model,
                    (1, cfg.video_channels, cfg.sequence_length, cfg.video_height, cfg.video_width),
                    steps=cfg.ode_steps,
                    device=device
                )
                
                # ä¿å­˜è§†é¢‘
                # sample_video: (B, C, T, H, W) -> (C, T, H, W) -> (T, C, H, W)
                sample_video = sample_video.squeeze(0)  # (C, T, H, W)
                sample_video = sample_video.transpose(0, 1)  # (T, C, H, W)
                video_path = os.path.join(cfg.out_dir, f"sample_epoch{epoch}.gif")
                save_video_tensor(sample_video, video_path, fps=cfg.fps)
                
        #         # ç”Ÿæˆå¤šä¸ªè§†é¢‘çš„ç½‘æ ¼
        #         videos = []
        #         for i in range(4):
        #             video = fm.sample(
        #                 model,
        #                 (1, cfg.video_channels, cfg.sequence_length, cfg.video_height, cfg.video_width),
        #                 steps=cfg.ode_steps,
        #                 device=device
        #             )
        #             # video: (B, C, T, H, W) -> (T, C, H, W)
        #             if video.size(0) == 1:
        #                 video = video.squeeze(0)  # (C, T, H, W)
        #             else:
        #                 video = video[0]  # (C, T, H, W)
        #             video = video.transpose(0, 1)  # (T, C, H, W)
        #             videos.append(video)
                
        #         grid_path = os.path.join(cfg.out_dir, f"grid_epoch{epoch}.gif")
        #         create_video_grid(videos, grid_path, fps=cfg.fps)
                
        #         print(f"ğŸ¬ æ ·æœ¬è§†é¢‘å·²ä¿å­˜: {video_path}")
        #         print(f"ğŸ¬ è§†é¢‘ç½‘æ ¼å·²ä¿å­˜: {grid_path}")
    
    # ä¿å­˜æ¨¡å‹
    torch.save(model.state_dict(), os.path.join(cfg.out_dir, "video_model.pt"))
    
    # ç»˜åˆ¶æŸå¤±æ›²çº¿
    plt.figure(figsize=(10, 6))
    plt.plot(losses)
    plt.title('Training Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.savefig(os.path.join(cfg.out_dir, 'loss_curve.png'))
    plt.close()
    
    print(f"ğŸ‰ è®­ç»ƒå®Œæˆ! æ¨¡å‹ä¿å­˜åœ¨: {cfg.out_dir}")

# -----------------------------
# Video Generation Function
# -----------------------------
def generate_videos():
    """ç”Ÿæˆè§†é¢‘"""
    device = torch.device(cfg.device)
    
    # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    model = VideoUNet(
        in_ch=cfg.video_channels,
        base_ch=cfg.base_ch,
        time_dim=cfg.time_dim,
        sequence_length=cfg.sequence_length
    ).to(device)
    
    model_path = os.path.join(cfg.out_dir, "video_model.pt")
    if os.path.exists(model_path):
        model.load_state_dict(torch.load(model_path, map_location=device))
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    else:
        print("âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹")
        return
    
    fm = VideoFlowMatching(device)
    
    print("ğŸ¬ å¼€å§‹ç”Ÿæˆè§†é¢‘...")
    
    # ç”Ÿæˆå¤šä¸ªè§†é¢‘
    num_videos = 8
    videos = []
    
    for i in range(num_videos):
        print(f"ç”Ÿæˆè§†é¢‘ {i+1}/{num_videos}")
        
        video = fm.sample(
            model,
            (1, cfg.video_channels, cfg.sequence_length, cfg.video_height, cfg.video_width),
            steps=cfg.ode_steps,
            device=device
        )
        
        # video: (B, C, T, H, W) -> (C, T, H, W) -> (T, C, H, W)
        video_processed = video.squeeze(0).transpose(0, 1)
        videos.append(video_processed)
        
        # ä¿å­˜å•ä¸ªè§†é¢‘
        video_path = os.path.join(cfg.out_dir, f"generated_video_{i+1}.gif")
        save_video_tensor(video_processed, video_path, fps=cfg.fps)
    
    # åˆ›å»ºè§†é¢‘ç½‘æ ¼
    grid_path = os.path.join(cfg.out_dir, "generated_videos_grid.gif")
    create_video_grid(videos, grid_path, fps=cfg.fps)
    
    print(f"ğŸ‰ è§†é¢‘ç”Ÿæˆå®Œæˆ! ä¿å­˜åœ¨: {cfg.out_dir}")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        if sys.argv[1] == "train":
            print("ğŸš€ å¼€å§‹è®­ç»ƒè§†é¢‘ç”Ÿæˆæ¨¡å‹")
            train_video_model()
        elif sys.argv[1] == "generate":
            print("ğŸ¬ å¼€å§‹ç”Ÿæˆè§†é¢‘")
            generate_videos()
        else:
            print("âŒ æœªçŸ¥å‚æ•°")
            print("ğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
            print("   python flow_matching_mario_video.py train     # è®­ç»ƒæ¨¡å‹")
            print("   python flow_matching_mario_video.py generate  # ç”Ÿæˆè§†é¢‘")
    else:
        print("ğŸš€ å¼€å§‹è®­ç»ƒè§†é¢‘ç”Ÿæˆæ¨¡å‹")
        train_video_model()
